{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef4425f",
   "metadata": {},
   "source": [
    "# Text File Ingestion with LangChain and PGVector\n",
    "\n",
    "This notebook demonstrates how to ingest text files (.txt) into a PostgreSQL vector database using LangChain and pgvector for RAG applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4587a88e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e71e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "from langchain_community.vectorstores import PGVector\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0315f6",
   "metadata": {},
   "source": [
    "## 2. Database Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd8db6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection string configured\n",
      "Connection: postgresql+psycopg2://rag_user:***@localhost:5432/rag_db\n",
      "‚úÖ Database connection test successful!\n"
     ]
    }
   ],
   "source": [
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'database': 'rag_db',\n",
    "    'user': 'rag_user',\n",
    "    'password': 'rag_password'\n",
    "}\n",
    "\n",
    "# Create connection string for LangChain\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "\n",
    "print(\"‚úÖ Database connection string configured\")\n",
    "print(f\"Connection: {CONNECTION_STRING.replace(DB_CONFIG['password'], '***')}\")\n",
    "\n",
    "# Test database connection\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Database connection test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16018d5",
   "metadata": {},
   "source": [
    "## 3. Initialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e696f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58530/3609688780.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings initialized\n",
      "Model: all-MiniLM-L6-v2\n",
      "‚úÖ Test embedding generated with dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Use OpenAI embeddings (requires API key)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Option 2: Use local sentence transformers (free, no API key needed)\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"‚úÖ Embeddings initialized\")\n",
    "print(f\"Model: {embeddings.model_name}\")\n",
    "\n",
    "# Test embeddings with a sample text\n",
    "test_text = \"This is a test document for embeddings.\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "print(f\"‚úÖ Test embedding generated with dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b870d6",
   "metadata": {},
   "source": [
    "## 4. Create PGVector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PGVector store for text ingestion\n",
    "COLLECTION_NAME = \"text_ingestion_docs\"\n",
    "\n",
    "try:\n",
    "    vectorstore = PGVector(\n",
    "        connection_string=CONNECTION_STRING,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        pre_delete_collection=True  # Delete existing collection if it exists\n",
    "    )\n",
    "    print(\"‚úÖ PGVector store created successfully!\")\n",
    "    print(f\"Collection name: {COLLECTION_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating vector store: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047e227",
   "metadata": {},
   "source": [
    "## 5. Load Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ef627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing text files\n",
    "TEXT_FILES_DIR = \"./text_files\"  # You can change this to your text files directory\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(TEXT_FILES_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Text files directory: {TEXT_FILES_DIR}\")\n",
    "\n",
    "# Option 1: Load all text files from directory\n",
    "try:\n",
    "    loader = DirectoryLoader(\n",
    "        TEXT_FILES_DIR,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"‚úÖ Loaded {len(documents)} text documents from directory\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading documents from directory: {e}\")\n",
    "    documents = []\n",
    "\n",
    "# Option 2: Load individual text files\n",
    "if not documents:\n",
    "    # Create sample text files for demonstration\n",
    "    sample_files = [\n",
    "        (\"sample_doc1.txt\", \"This is a sample document about artificial intelligence. AI refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\"),\n",
    "        (\"sample_doc2.txt\", \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed. It uses algorithms to identify patterns in data.\"),\n",
    "        (\"sample_doc3.txt\", \"Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and humans through natural language.\")\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for filename, content in sample_files:\n",
    "        filepath = os.path.join(TEXT_FILES_DIR, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        # Load the created file\n",
    "        loader = TextLoader(filepath)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata['source'] = filename\n",
    "            doc.metadata['filepath'] = filepath\n",
    "        documents.extend(docs)\n",
    "\n",
    "    print(f\"‚úÖ Created and loaded {len(documents)} sample text documents\")\n",
    "\n",
    "# Display loaded documents info\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('source', 'Unknown')}: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f06ab4",
   "metadata": {},
   "source": [
    "## 6. Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks for better retrieval\n",
    "CHUNK_SIZE = 1000  # Characters per chunk\n",
    "CHUNK_OVERLAP = 200  # Overlap between chunks\n",
    "\n",
    "print(f\"üìè Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"üîó Chunk overlap: {CHUNK_OVERLAP} characters\")\n",
    "\n",
    "# Option 1: Use RecursiveCharacterTextSplitter (recommended for text)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Try splitting by paragraphs, lines, words, characters\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Option 2: Use CharacterTextSplitter (simpler)\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     chunk_size=CHUNK_SIZE,\n",
    "#     chunk_overlap=CHUNK_OVERLAP,\n",
    "#     separator=\"\\n\"\n",
    "# )\n",
    "\n",
    "# Split the documents\n",
    "try:\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    print(f\"‚úÖ Split {len(documents)} documents into {len(split_documents)} chunks\")\n",
    "\n",
    "    # Display chunk statistics\n",
    "    chunk_lengths = [len(doc.page_content) for doc in split_documents]\n",
    "    print(\"üìä Chunk Statistics:\")\n",
    "    print(f\"   Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"   Min chunk length: {min(chunk_lengths)} characters\")\n",
    "    print(f\"   Max chunk length: {max(chunk_lengths)} characters\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error splitting documents: {e}\")\n",
    "    split_documents = documents  # Fallback to original documents\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\nüìÑ Sample chunks:\")\n",
    "for i, chunk in enumerate(split_documents[:3], 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Source: {chunk.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content preview: {chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64447bf",
   "metadata": {},
   "source": [
    "## 7. Add Documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add split documents to vector store\n",
    "print(\"üöÄ Starting document ingestion...\")\n",
    "print(f\"üìä Adding {len(split_documents)} document chunks to vector store\")\n",
    "\n",
    "try:\n",
    "    # Add documents in batches to handle large datasets\n",
    "    batch_size = 100\n",
    "    total_added = 0\n",
    "\n",
    "    for i in range(0, len(split_documents), batch_size):\n",
    "        batch = split_documents[i:i + batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "        total_added += len(batch)\n",
    "        print(f\"‚úÖ Added batch {i//batch_size + 1}: {len(batch)} documents (Total: {total_added})\")\n",
    "\n",
    "    print(f\"\\nüéâ Successfully ingested {total_added} document chunks!\")\n",
    "    print(f\"üìö Collection: {COLLECTION_NAME}\")\n",
    "    print(f\"ü§ñ Embedding model: {embeddings.model_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error adding documents to vector store: {e}\")\n",
    "\n",
    "# Optional: Add metadata to track ingestion\n",
    "try:\n",
    "    ingestion_metadata = {\n",
    "        \"ingestion_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        \"total_documents\": len(split_documents),\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "        \"embedding_model\": embeddings.model_name,\n",
    "        \"collection_name\": COLLECTION_NAME\n",
    "    }\n",
    "\n",
    "    # You could store this metadata in the database or a separate file\n",
    "    print(\"üìã Ingestion metadata:\")\n",
    "    for key, value in ingestion_metadata.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not create ingestion metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b59686",
   "metadata": {},
   "source": [
    "## 8. Verify Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c64d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ingestion by querying the vector store\n",
    "print(\"üîç Verifying document ingestion...\")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What is natural language processing?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    try:\n",
    "        results = vectorstore.similarity_search(query, k=2)\n",
    "        print(f\"\\nüîç Query: '{query}'\")\n",
    "        print(f\"üìä Found {len(results)} similar documents:\")\n",
    "\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"   {i}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            print(f\"      Content: {doc.page_content[:100]}...\")\n",
    "            print(f\"      Similarity: {doc.metadata.get('score', 'N/A')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying for '{query}': {e}\")\n",
    "\n",
    "# Get collection statistics\n",
    "print(\"\\nüìä Collection Statistics:\")\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get document count\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM langchain_pg_embedding\n",
    "        WHERE collection_id = (\n",
    "            SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    doc_count = cursor.fetchone()[0]\n",
    "    print(f\"   Documents in collection: {doc_count}\")\n",
    "    print(f\"   Collection name: {COLLECTION_NAME}\")\n",
    "\n",
    "    # Get embedding dimension\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT embedding\n",
    "        FROM langchain_pg_embedding\n",
    "        WHERE collection_id = (\n",
    "            SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'\n",
    "        )\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "\n",
    "    sample_embedding = cursor.fetchone()\n",
    "    if sample_embedding:\n",
    "        embedding_dim = len(sample_embedding[0])\n",
    "        print(f\"   Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error getting statistics: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ingestion verification completed!\")\n",
    "print(\"\\nüí° Next steps:\")\n",
    "print(\"- Use this collection for RAG applications\")\n",
    "print(\"- Fine-tune chunk size and overlap for your use case\")\n",
    "print(\"- Add more text files to expand your knowledge base\")\n",
    "print(\"- Implement document filtering and metadata enrichment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
